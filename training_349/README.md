---

# Инструкция по классификации текстов

В этом ноутбуке мы обучим модель машинного обучения автоматически распознавать категорию текстов. Например, у нас есть описание вакансий, и мы хотим, чтобы модель могла автоматически определять их категорию, такую как «Айти», «Маркетинг», «Финансы» и т.д. 

## Шаги работы

1. **Установка необходимых библиотек**
2. **Загрузка данных и предобработка текста**
3. **Токенизация текста**
4. **Загрузка модели**
5. **Предсказание**
6. **Сохранение результатов**

---

## 1. Установка необходимых библиотек

Для выполнения ноутбука нам понадобятся следующие библиотеки:

```bash
pip install pandas torch transformers tqdm scikit-learn
```

### Зачем нужны эти библиотеки?

- **pandas** — для работы с таблицами и удобного хранения данных.
- **torch** — основа для работы с нейросетями, часть PyTorch.
- **transformers** — библиотека от Hugging Face с готовыми моделями и токенизаторами.
- **tqdm** — показывает прогресс выполнения, что полезно при обработке большого объема данных.
- **scikit-learn** — для машинного обучения, в данном случае, чтобы закодировать категории.

---

## 2. Загрузка данных и предобработка текста

На этом этапе мы загружаем файл с текстом и очищаем его, чтобы сделать данные более понятными для модели. Например, описание вакансий часто содержит лишние символы, заглавные буквы и пробелы. 

**Цель:** Привести текст к единому формату, который будет легче обработать модели.

### Шаги:

1. Загрузим файл с данными в виде таблицы.
2. Очистим текст: удалим лишние символы, пробелы и переведем в строчные буквы.

### Код:

```python
import pandas as pd
import re

# Загрузим данные из Excel-файла
df = pd.read_excel('dataset/df_3_final_realy_really.xlsx')

# Функция для очистки текста
def preprocess(text):
    text = text.lower()  # Переводим текст в нижний регистр
    text = re.sub(r'[^\w\s]', '', text)  # Удаляем все, кроме букв и пробелов
    return text.strip()  # Убираем пробелы в начале и конце текста

# Применяем предобработку ко всем описаниям в столбце 'description'
df['description'] = df['description'].apply(preprocess)
```

### Что делает этот код?

1. **df = pd.read_excel(...)** — загружает данные из Excel-файла в DataFrame `df`, позволяя нам работать с таблицей.
2. **preprocess(text)** — функция для очистки текста:
   - `text.lower()` — переводит текст в строчные буквы, чтобы все слова имели единый регистр.
   - `re.sub(r'[^\w\s]', '', text)` — убирает все символы, кроме букв и пробелов.
   - `text.strip()` — удаляет лишние пробелы в начале и конце строки.

После этого шага наш текст становится более чистым и унифицированным, что упрощает дальнейшую обработку для модели. 



## 3. Токенизация текста

После предобработки нам нужно перевести текст в числовой формат, потому что модель не понимает слова — ей нужны числа. Токенизация — это процесс, который разбивает текст на токены (словесные или буквенные фрагменты) и переводит их в числовое представление.

### Шаги:

1. Загружаем токенизатор — инструмент, который будет переводить текст в числа.
2. Применяем токенизацию к каждому описанию в нашем наборе данных.

```python
from transformers import AutoTokenizer

# Загружаем токенизатор из библиотеки Hugging Face
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Токенизируем описания
encodings = tokenizer(df['description'].tolist(), padding='max_length', truncation=True, max_length=512, return_tensors="pt")
```

### Что здесь происходит?

- AutoTokenizer.from_pretrained('bert-base-uncased') загружает токенизатор модели BERT, который умеет разбивать текст на токены.
- padding='max_length' добавляет «пустые» токены в конце каждого описания, чтобы все были одинаковой длины.
- truncation=True обрезает слишком длинные тексты, чтобы они не превышали длину 512 символов.
- return_tensors="pt" преобразует данные в формат PyTorch, который понимает модель.

## 4. Загрузка модели

Теперь, когда текст готов, мы загрузим модель. У нас уже есть обученная модель, сохраненная в файле, и теперь мы ее загрузим, чтобы использовать для предсказаний.

### Шаги:

1. Загрузим модель из файла.
2. Переведем ее в режим предсказания, так как нам не нужно её дообучать.

```python
import torch

# Указываем путь к модели
model_path = 'checkpoint.pth_epoch_9.pth'
model = torch.load(model_path)
model.eval()  # Переводим модель в режим предсказания
```

### Объяснение:

- torch.load(model_path) загружает обученную модель.
- model.eval() переводит модель в режим предсказания, отключая функции, которые нужны только при обучении.

## 5. Предсказание

Теперь мы можем использовать модель для того, чтобы классифицировать каждый текст. Это значит, что модель будет определять, к какой категории относится каждое описание.
Шаги:

1. Создадим специальный класс, чтобы обрабатывать наши данные батчами (небольшими группами).
2. Напишем функцию, которая прогоняет каждый батч через модель и сохраняет предсказания.

```python
from torch.utils.data import DataLoader, Dataset

# Класс для обработки данных
class DescriptionDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings['input_ids'])

# Создаем DataLoader для загрузки данных по батчам
description_loader = DataLoader(DescriptionDataset(encodings), batch_size=16, shuffle=False)

# Функция для предсказания
def predict(model, data_loader):
    predictions = []
    with torch.no_grad():  # Отключаем расчет градиентов
        for batch in data_loader:
            outputs = model(**batch)
            preds = outputs.logits.argmax(dim=1)  # Находим класс с наибольшей вероятностью
            predictions.extend(preds.cpu().numpy())
    return predictions

# Делаем предсказания
predicted_classes = predict(model, description_loader)
```

### Объяснение:

- DescriptionDataset и DataLoader позволяют нам разбить данные на части, чтобы быстрее обрабатывать их.
- predict обходит каждую часть данных, прогоняет её через модель и сохраняет результаты.
- torch.no_grad() отключает вычисление градиентов, чтобы ускорить предсказания и уменьшить использование памяти.

## 6. Сохранение результатов

Мы добавим результаты обратно в наш DataFrame, чтобы их было удобно просмотреть, а потом сохраним их в файл Excel.

```python
# Добавляем предсказанные категории в DataFrame
df['predicted_class'] = predicted_classes

# Сохраняем DataFrame в Excel
df.to_excel('classified_descriptions.xlsx', index=False)
```
---

